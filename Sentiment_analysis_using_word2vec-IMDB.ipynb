{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\nitisha.reddy\\Documents\\Intellipaat\\Datasets\\Movie_recommendation\\IMDB Dataset.csv\",encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 50000/50000 [17:32:07<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "for i in tqdm(range(0,data.shape[0])):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', data['review'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one reviewer mentioned watching oz episode hooked right exactly happened br br first thing struck oz brutality unflinching scene violence set right word go trust show faint hearted timid show pull punch regard drug sex violence hardcore classic use word br br called oz nickname given oswald maximum security state penitentary focus mainly emerald city experimental section prison cell glass front face inwards privacy high agenda em city home many aryan muslim gangsta latino christian italian irish scuffle death stare dodgy dealing shady agreement never far away br br would say main appeal show due fact go show dare forget pretty picture painted mainstream audience forget charm forget romance oz mess around first episode ever saw struck nasty surreal say ready watched developed taste oz got accustomed high level graphic violence violence injustice crooked guard sold nickel inmate kill order get away well mannered middle class inmate turned prison bitch due lack street skill prison experience watching oz may become comfortable uncomfortable viewing thats get touch darker side'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [00:16<00:00, 3015.00it/s]\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for i in tqdm(corpus):\n",
    "    sent_token = sent_tokenize(i)\n",
    "    for j in sent_token:\n",
    "        words.append(simple_preprocess(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model1 = gensim.models.Word2Vec(words,window=5,min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgword2vec(doc):\n",
    "    return np.mean([model1.wv[word] for word in doc if word in model1.wv.index_to_key],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54976"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model1.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 50000/50000 [17:49<00:00, 46.73it/s]\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "for i in tqdm(range(len(words))):\n",
    "    X.append(avgword2vec(words[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2file as bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressed_pickle(title, data):\n",
    "    with bz2.BZ2File(title + \".pbz2\", \"w\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.word2vec.Word2Vec"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=model1.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(model1.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    z=[]\n",
    "    for j in range(100):\n",
    "        z.append(float(\"{0:.2f}\".format(x[i][j])))\n",
    "    \n",
    "    model1.wv[i]=np.array(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2 , -0.25, -1.35, -0.23,  0.92,  0.79,  1.46,  0.33, -0.06,\n",
       "        0.2 ,  0.68,  1.75, -1.39,  1.92,  0.07, -0.35,  1.22,  1.06,\n",
       "       -3.66,  0.18, -0.08,  0.24,  2.13, -0.57,  1.12, -0.23, -0.79,\n",
       "        1.61, -0.25,  1.5 , -0.57, -1.85,  0.91, -1.21, -1.37,  0.63,\n",
       "        1.79,  0.36,  2.45,  1.81,  0.39, -0.47,  0.07, -0.57,  0.36,\n",
       "        0.63,  0.14,  0.97, -1.17, -0.65,  1.19, -0.44, -0.51, -0.92,\n",
       "       -0.4 , -1.63,  0.65, -1.18, -0.52,  0.69,  0.52, -0.68, -1.1 ,\n",
       "       -0.72, -0.21,  1.4 ,  0.28, -0.07, -0.52,  0.83,  0.93,  1.6 ,\n",
       "       -2.6 ,  1.24,  2.18, -1.61, -0.11,  1.76,  0.83, -0.56, -1.74,\n",
       "        1.08,  0.24,  0.98, -0.64,  0.21,  1.45, -2.26, -2.14,  0.36,\n",
       "        1.07, -1.4 ,  0.7 , -0.33,  1.62,  0.2 ,  0.44, -0.31, -0.97,\n",
       "        0.6 ], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2=model1.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02, -0.32, -1.14, -0.02,  0.4 ,  1.28,  0.33,  1.32,  0.83,\n",
       "        0.5 , -0.02,  1.44, -0.51,  1.57, -0.23, -0.9 ,  1.7 ,  1.11,\n",
       "       -3.61,  0.01,  0.14,  0.29,  1.73,  0.12,  1.73, -0.23,  0.35,\n",
       "        1.73, -1.05,  1.33, -1.09,  0.28,  0.32, -1.14, -1.79,  0.97,\n",
       "        1.55, -0.13,  1.91,  0.69, -0.28, -0.18, -0.75, -0.9 ,  0.63,\n",
       "        1.57,  0.17,  1.02, -1.58, -1.32,  1.98, -0.26, -1.64, -0.84,\n",
       "       -0.56, -1.93,  0.6 , -0.89, -1.06,  0.04,  1.41, -1.75, -0.75,\n",
       "       -0.47, -0.83,  1.22,  0.21,  0.24, -0.95, -0.07,  0.95,  1.3 ,\n",
       "       -1.9 ,  1.22,  1.09, -0.85,  1.05,  1.44,  0.28, -0.12, -1.65,\n",
       "        0.92,  0.29,  0.4 , -0.51,  0.04,  0.15, -2.21, -1.37,  1.48,\n",
       "        0.76, -0.38, -0.09, -0.71,  1.57,  0.92, -0.1 ,  0.13, -1.03,\n",
       "        0.46], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2['film']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_pickle(\"word2vec_bz2file_decimal\", model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgword2vec1(doc):\n",
    "    return np.mean([model2[word] for word in doc if word in model2.index_to_key],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 50000/50000 [31:30<00:00, 26.44it/s]\n"
     ]
    }
   ],
   "source": [
    "X1=[]\n",
    "for i in tqdm(range(len(words))):\n",
    "    X1.append(avgword2vec1(words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['br',\n",
       " 'movie',\n",
       " 'film',\n",
       " 'one',\n",
       " 'like',\n",
       " 'time',\n",
       " 'good',\n",
       " 'character',\n",
       " 'story',\n",
       " 'even',\n",
       " 'get',\n",
       " 'would',\n",
       " 'make',\n",
       " 'see',\n",
       " 'really',\n",
       " 'scene',\n",
       " 'well',\n",
       " 'much',\n",
       " 'bad',\n",
       " 'people',\n",
       " 'great',\n",
       " 'also',\n",
       " 'first',\n",
       " 'show',\n",
       " 'way',\n",
       " 'thing',\n",
       " 'made',\n",
       " 'life',\n",
       " 'could',\n",
       " 'think',\n",
       " 'go',\n",
       " 'know',\n",
       " 'watch',\n",
       " 'love',\n",
       " 'plot',\n",
       " 'actor',\n",
       " 'two',\n",
       " 'many',\n",
       " 'seen',\n",
       " 'year',\n",
       " 'say',\n",
       " 'end',\n",
       " 'never',\n",
       " 'acting',\n",
       " 'look',\n",
       " 'best',\n",
       " 'little',\n",
       " 'ever',\n",
       " 'man',\n",
       " 'better',\n",
       " 'take',\n",
       " 'come',\n",
       " 'work',\n",
       " 'still',\n",
       " 'part',\n",
       " 'something',\n",
       " 'director',\n",
       " 'find',\n",
       " 'want',\n",
       " 'back',\n",
       " 'give',\n",
       " 'lot',\n",
       " 'real',\n",
       " 'guy',\n",
       " 'watching',\n",
       " 'performance',\n",
       " 'woman',\n",
       " 'play',\n",
       " 'old',\n",
       " 'funny',\n",
       " 'though',\n",
       " 'another',\n",
       " 'actually',\n",
       " 'role',\n",
       " 'nothing',\n",
       " 'going',\n",
       " 'new',\n",
       " 'every',\n",
       " 'girl',\n",
       " 'day',\n",
       " 'world',\n",
       " 'point',\n",
       " 'cast',\n",
       " 'horror',\n",
       " 'minute',\n",
       " 'comedy',\n",
       " 'thought',\n",
       " 'fact',\n",
       " 'feel',\n",
       " 'quite',\n",
       " 'pretty',\n",
       " 'star',\n",
       " 'action',\n",
       " 'around',\n",
       " 'seems',\n",
       " 'young',\n",
       " 'big',\n",
       " 'however',\n",
       " 'got',\n",
       " 'enough',\n",
       " 'right',\n",
       " 'long',\n",
       " 'line',\n",
       " 'fan',\n",
       " 'may',\n",
       " 'friend',\n",
       " 'bit',\n",
       " 'series',\n",
       " 'must',\n",
       " 'music',\n",
       " 'set',\n",
       " 'without',\n",
       " 'original',\n",
       " 'family',\n",
       " 'saw',\n",
       " 'always',\n",
       " 'almost',\n",
       " 'script',\n",
       " 'interesting',\n",
       " 'done',\n",
       " 'whole',\n",
       " 'least',\n",
       " 'kid',\n",
       " 'try',\n",
       " 'shot',\n",
       " 'far',\n",
       " 'kind',\n",
       " 'effect',\n",
       " 'last',\n",
       " 'making',\n",
       " 'might',\n",
       " 'anything',\n",
       " 'since',\n",
       " 'reason',\n",
       " 'tv',\n",
       " 'start',\n",
       " 'book',\n",
       " 'probably',\n",
       " 'child',\n",
       " 'put',\n",
       " 'place',\n",
       " 'away',\n",
       " 'yet',\n",
       " 'moment',\n",
       " 'let',\n",
       " 'fun',\n",
       " 'anyone',\n",
       " 'worst',\n",
       " 'american',\n",
       " 'sure',\n",
       " 'rather',\n",
       " 'hard',\n",
       " 'audience',\n",
       " 'idea',\n",
       " 'played',\n",
       " 'screen',\n",
       " 'found',\n",
       " 'need',\n",
       " 'dvd',\n",
       " 'turn',\n",
       " 'war',\n",
       " 'tell',\n",
       " 'looking',\n",
       " 'although',\n",
       " 'especially',\n",
       " 'believe',\n",
       " 'ending',\n",
       " 'episode',\n",
       " 'course',\n",
       " 'trying',\n",
       " 'everything',\n",
       " 'mean',\n",
       " 'job',\n",
       " 'maybe',\n",
       " 'three',\n",
       " 'worth',\n",
       " 'different',\n",
       " 'someone',\n",
       " 'main',\n",
       " 'sense',\n",
       " 'problem',\n",
       " 'boy',\n",
       " 'night',\n",
       " 'version',\n",
       " 'money',\n",
       " 'true',\n",
       " 'watched',\n",
       " 'second',\n",
       " 'keep',\n",
       " 'everyone',\n",
       " 'together',\n",
       " 'help',\n",
       " 'high',\n",
       " 'mind',\n",
       " 'black',\n",
       " 'half',\n",
       " 'death',\n",
       " 'instead',\n",
       " 'hour',\n",
       " 'said',\n",
       " 'wife',\n",
       " 'special',\n",
       " 'le',\n",
       " 'later',\n",
       " 'john',\n",
       " 'laugh',\n",
       " 'house',\n",
       " 'seem',\n",
       " 'beautiful',\n",
       " 'left',\n",
       " 'short',\n",
       " 'seeing',\n",
       " 'lead',\n",
       " 'sound',\n",
       " 'excellent',\n",
       " 'name',\n",
       " 'classic',\n",
       " 'eye',\n",
       " 'father',\n",
       " 'face',\n",
       " 'else',\n",
       " 'budget',\n",
       " 'production',\n",
       " 'read',\n",
       " 'viewer',\n",
       " 'piece',\n",
       " 'top',\n",
       " 'used',\n",
       " 'nice',\n",
       " 'simply',\n",
       " 'poor',\n",
       " 'completely',\n",
       " 'men',\n",
       " 'home',\n",
       " 'camera',\n",
       " 'human',\n",
       " 'picture',\n",
       " 'along',\n",
       " 'video',\n",
       " 'song',\n",
       " 'head',\n",
       " 'dead',\n",
       " 'word',\n",
       " 'hollywood',\n",
       " 'hand',\n",
       " 'school',\n",
       " 'either',\n",
       " 'couple',\n",
       " 'boring',\n",
       " 'wrong',\n",
       " 'low',\n",
       " 'enjoy',\n",
       " 'use',\n",
       " 'given',\n",
       " 'full',\n",
       " 'stupid',\n",
       " 'rest',\n",
       " 'next',\n",
       " 'writer',\n",
       " 'sex',\n",
       " 'truly',\n",
       " 'awful',\n",
       " 'kill',\n",
       " 'run',\n",
       " 'hope',\n",
       " 'mr',\n",
       " 'game',\n",
       " 'person',\n",
       " 'recommend',\n",
       " 'style',\n",
       " 'title',\n",
       " 'mother',\n",
       " 'terrible',\n",
       " 'remember',\n",
       " 'getting',\n",
       " 'sort',\n",
       " 'came',\n",
       " 'dialogue',\n",
       " 'understand',\n",
       " 'case',\n",
       " 'perhaps',\n",
       " 'flick',\n",
       " 'act',\n",
       " 'joke',\n",
       " 'wonderful',\n",
       " 'care',\n",
       " 'playing',\n",
       " 'attempt',\n",
       " 'small',\n",
       " 'others',\n",
       " 'fall',\n",
       " 'perfect',\n",
       " 'review',\n",
       " 'sequence',\n",
       " 'early',\n",
       " 'often',\n",
       " 'art',\n",
       " 'drama',\n",
       " 'written',\n",
       " 'definitely',\n",
       " 'brother',\n",
       " 'killer',\n",
       " 'cinema',\n",
       " 'actress',\n",
       " 'example',\n",
       " 'quality',\n",
       " 'went',\n",
       " 'finally',\n",
       " 'feeling',\n",
       " 'yes',\n",
       " 'absolutely',\n",
       " 'live',\n",
       " 'oh',\n",
       " 'certainly',\n",
       " 'lost',\n",
       " 'car',\n",
       " 'liked',\n",
       " 'become',\n",
       " 'entertaining',\n",
       " 'worse',\n",
       " 'side',\n",
       " 'loved',\n",
       " 'called',\n",
       " 'white',\n",
       " 'waste',\n",
       " 'matter',\n",
       " 'overall',\n",
       " 'direction',\n",
       " 'felt',\n",
       " 'son',\n",
       " 'based',\n",
       " 'entire',\n",
       " 'several',\n",
       " 'supposed',\n",
       " 'heart',\n",
       " 'feature',\n",
       " 'beginning',\n",
       " 'comment',\n",
       " 'lack',\n",
       " 'begin',\n",
       " 'hero',\n",
       " 'favorite',\n",
       " 'dark',\n",
       " 'fight',\n",
       " 'type',\n",
       " 'murder',\n",
       " 'totally',\n",
       " 'evil',\n",
       " 'humor',\n",
       " 'wanted',\n",
       " 'seemed',\n",
       " 'despite',\n",
       " 'guess',\n",
       " 'final',\n",
       " 'change',\n",
       " 'voice',\n",
       " 'already',\n",
       " 'throughout',\n",
       " 'relationship',\n",
       " 'hit',\n",
       " 'becomes',\n",
       " 'number',\n",
       " 'meet',\n",
       " 'genre',\n",
       " 'unfortunately',\n",
       " 'able',\n",
       " 'michael',\n",
       " 'history',\n",
       " 'experience',\n",
       " 'cut',\n",
       " 'god',\n",
       " 'stop',\n",
       " 'today',\n",
       " 'town',\n",
       " 'daughter',\n",
       " 'writing',\n",
       " 'fine',\n",
       " 'age',\n",
       " 'city',\n",
       " 'horrible',\n",
       " 'close',\n",
       " 'amazing',\n",
       " 'group',\n",
       " 'call',\n",
       " 'move',\n",
       " 'power',\n",
       " 'talent',\n",
       " 'past',\n",
       " 'etc',\n",
       " 'event',\n",
       " 'stuff',\n",
       " 'enjoyed',\n",
       " 'brilliant',\n",
       " 'behind',\n",
       " 'gave',\n",
       " 'robert',\n",
       " 'credit',\n",
       " 'directed',\n",
       " 'level',\n",
       " 'situation',\n",
       " 'late',\n",
       " 'add',\n",
       " 'save',\n",
       " 'theme',\n",
       " 'body',\n",
       " 'expect',\n",
       " 'stand',\n",
       " 'chance',\n",
       " 'soon',\n",
       " 'wonder',\n",
       " 'obviously',\n",
       " 'score',\n",
       " 'sometimes',\n",
       " 'self',\n",
       " 'thinking',\n",
       " 'killed',\n",
       " 'question',\n",
       " 'decent',\n",
       " 'view',\n",
       " 'highly',\n",
       " 'light',\n",
       " 'blood',\n",
       " 'anyway',\n",
       " 'known',\n",
       " 'took',\n",
       " 'heard',\n",
       " 'happens',\n",
       " 'except',\n",
       " 'coming',\n",
       " 'element',\n",
       " 'monster',\n",
       " 'country',\n",
       " 'wish',\n",
       " 'career',\n",
       " 'talk',\n",
       " 'slow',\n",
       " 'documentary',\n",
       " 'rating',\n",
       " 'novel',\n",
       " 'husband',\n",
       " 'leave',\n",
       " 'order',\n",
       " 'police',\n",
       " 'told',\n",
       " 'interest',\n",
       " 'extremely',\n",
       " 'violence',\n",
       " 'involved',\n",
       " 'reality',\n",
       " 'strong',\n",
       " 'ok',\n",
       " 'hilarious',\n",
       " 'cannot',\n",
       " 'crap',\n",
       " 'hell',\n",
       " 'happen',\n",
       " 'effort',\n",
       " 'living',\n",
       " 'sister',\n",
       " 'particularly',\n",
       " 'including',\n",
       " 'opinion',\n",
       " 'looked',\n",
       " 'cool',\n",
       " 'david',\n",
       " 'musical',\n",
       " 'please',\n",
       " 'simple',\n",
       " 'twist',\n",
       " 'sequel',\n",
       " 'gore',\n",
       " 'obvious',\n",
       " 'complete',\n",
       " 'happened',\n",
       " 'serious',\n",
       " 'james',\n",
       " 'lady',\n",
       " 'room',\n",
       " 'deal',\n",
       " 'theater',\n",
       " 'ago',\n",
       " 'female',\n",
       " 'thriller',\n",
       " 'taken',\n",
       " 'shown',\n",
       " 'english',\n",
       " 'cop',\n",
       " 'seriously',\n",
       " 'released',\n",
       " 'zombie',\n",
       " 'value',\n",
       " 'opening',\n",
       " 'dog',\n",
       " 'miss',\n",
       " 'alone',\n",
       " 'none',\n",
       " 'spoiler',\n",
       " 'across',\n",
       " 'dream',\n",
       " 'possible',\n",
       " 'sad',\n",
       " 'exactly',\n",
       " 'saying',\n",
       " 'usually',\n",
       " 'cinematography',\n",
       " 'dialog',\n",
       " 'running',\n",
       " 'annoying',\n",
       " 'huge',\n",
       " 'class',\n",
       " 'comic',\n",
       " 'open',\n",
       " 'major',\n",
       " 'whose',\n",
       " 'producer',\n",
       " 'taking',\n",
       " 'ridiculous',\n",
       " 'stay',\n",
       " 'local',\n",
       " 'middle',\n",
       " 'important',\n",
       " 'jack',\n",
       " 'four',\n",
       " 'started',\n",
       " 'message',\n",
       " 'tale',\n",
       " 'turned',\n",
       " 'scary',\n",
       " 'team',\n",
       " 'mostly',\n",
       " 'usual',\n",
       " 'beyond',\n",
       " 'happy',\n",
       " 'rock',\n",
       " 'silly',\n",
       " 'hate',\n",
       " 'somewhat',\n",
       " 'attention',\n",
       " 'street',\n",
       " 'talking',\n",
       " 'knew',\n",
       " 'surprise',\n",
       " 'single',\n",
       " 'king',\n",
       " 'disappointed',\n",
       " 'return',\n",
       " 'parent',\n",
       " 'strange',\n",
       " 'apparently',\n",
       " 'non',\n",
       " 'television',\n",
       " 'due',\n",
       " 'modern',\n",
       " 'upon',\n",
       " 'figure',\n",
       " 'basically',\n",
       " 'cheap',\n",
       " 'release',\n",
       " 'image',\n",
       " 'member',\n",
       " 'earth',\n",
       " 'mention',\n",
       " 'oscar',\n",
       " 'result',\n",
       " 'crime',\n",
       " 'season',\n",
       " 'george',\n",
       " 'clich',\n",
       " 'state',\n",
       " 'subject',\n",
       " 'clearly',\n",
       " 'space',\n",
       " 'ten',\n",
       " 'drug',\n",
       " 'five',\n",
       " 'western',\n",
       " 'british',\n",
       " 'filmmaker',\n",
       " 'gun',\n",
       " 'french',\n",
       " 'mystery',\n",
       " 'moving',\n",
       " 'entertainment',\n",
       " 'fast',\n",
       " 'straight',\n",
       " 'soundtrack',\n",
       " 'hold',\n",
       " 'future',\n",
       " 'predictable',\n",
       " 'die',\n",
       " 'form',\n",
       " 'viewing',\n",
       " 'villain',\n",
       " 'whether',\n",
       " 'doubt',\n",
       " 'killing',\n",
       " 'romantic',\n",
       " 'break',\n",
       " 'easily',\n",
       " 'enjoyable',\n",
       " 'near',\n",
       " 'peter',\n",
       " 'appears',\n",
       " 'buy',\n",
       " 'bring',\n",
       " 'aspect',\n",
       " 'giving',\n",
       " 'note',\n",
       " 'similar',\n",
       " 'supporting',\n",
       " 'adult',\n",
       " 'soldier',\n",
       " 'working',\n",
       " 'clear',\n",
       " 'within',\n",
       " 'filmed',\n",
       " 'showing',\n",
       " 'doctor',\n",
       " 'bunch',\n",
       " 'emotion',\n",
       " 'dull',\n",
       " 'suspense',\n",
       " 'surprised',\n",
       " 'storyline',\n",
       " 'easy',\n",
       " 'sorry',\n",
       " 'tried',\n",
       " 'certain',\n",
       " 'force',\n",
       " 'dance',\n",
       " 'present',\n",
       " 'setting',\n",
       " 'among',\n",
       " 'named',\n",
       " 'standard',\n",
       " 'check',\n",
       " 'student',\n",
       " 'period',\n",
       " 'general',\n",
       " 'victim',\n",
       " 'gone',\n",
       " 'dr',\n",
       " 'using',\n",
       " 'rent',\n",
       " 'th',\n",
       " 'typical',\n",
       " 'editing',\n",
       " 'material',\n",
       " 'avoid',\n",
       " 'cartoon',\n",
       " 'detail',\n",
       " 'richard',\n",
       " 'fantastic',\n",
       " 'okay',\n",
       " 'realistic',\n",
       " 'nearly',\n",
       " 'kept',\n",
       " 'battle',\n",
       " 'red',\n",
       " 'animation',\n",
       " 'greatest',\n",
       " 'alien',\n",
       " 'famous',\n",
       " 'paul',\n",
       " 'imagine',\n",
       " 'actual',\n",
       " 'mark',\n",
       " 'follow',\n",
       " 'premise',\n",
       " 'issue',\n",
       " 'wait',\n",
       " 'water',\n",
       " 'tom',\n",
       " 'believable',\n",
       " 'box',\n",
       " 'truth',\n",
       " 'brought',\n",
       " 'vampire',\n",
       " 'romance',\n",
       " 'somehow',\n",
       " 'forget',\n",
       " 'america',\n",
       " 'masterpiece',\n",
       " 'male',\n",
       " 'copy',\n",
       " 'fit',\n",
       " 'third',\n",
       " 'offer',\n",
       " 'imdb',\n",
       " 'pay',\n",
       " 'average',\n",
       " 'de',\n",
       " 'weak',\n",
       " 'background',\n",
       " 'escape',\n",
       " 'hear',\n",
       " 'society',\n",
       " 'sit',\n",
       " 'atmosphere',\n",
       " 'stage',\n",
       " 'cover',\n",
       " 'york',\n",
       " 'deep',\n",
       " 'german',\n",
       " 'cause',\n",
       " 'baby',\n",
       " 'fi',\n",
       " 'gay',\n",
       " 'eventually',\n",
       " 'learn',\n",
       " 'sci',\n",
       " 'whatever',\n",
       " 'expected',\n",
       " 'particular',\n",
       " 'secret',\n",
       " 'lame',\n",
       " 'indeed',\n",
       " 'poorly',\n",
       " 'fire',\n",
       " 'crew',\n",
       " 'walk',\n",
       " 'free',\n",
       " 'shame',\n",
       " 'studio',\n",
       " 'leaf',\n",
       " 'rate',\n",
       " 'choice',\n",
       " 'hot',\n",
       " 'wood',\n",
       " 'adventure',\n",
       " 'week',\n",
       " 'decided',\n",
       " 'party',\n",
       " 'screenplay',\n",
       " 'lee',\n",
       " 'reading',\n",
       " 'air',\n",
       " 'island',\n",
       " 'difficult',\n",
       " 'girlfriend',\n",
       " 'beauty',\n",
       " 'needed',\n",
       " 'footage',\n",
       " 'acted',\n",
       " 'focus',\n",
       " 'sexual',\n",
       " 'unless',\n",
       " 'emotional',\n",
       " 'possibly',\n",
       " 'accent',\n",
       " 'write',\n",
       " 'lover',\n",
       " 'forced',\n",
       " 'memorable',\n",
       " 'became',\n",
       " 'nature',\n",
       " 'otherwise',\n",
       " 'match',\n",
       " 'plus',\n",
       " 'forward',\n",
       " 'bill',\n",
       " 'location',\n",
       " 'touch',\n",
       " 'animal',\n",
       " 'cheesy',\n",
       " 'superb',\n",
       " 'fantasy',\n",
       " 'inside',\n",
       " 'interested',\n",
       " 'development',\n",
       " 'perfectly',\n",
       " 'crazy',\n",
       " 'weird',\n",
       " 'badly',\n",
       " 'japanese',\n",
       " 'previous',\n",
       " 'mess',\n",
       " 'personal',\n",
       " 'maker',\n",
       " 'pick',\n",
       " 'quickly',\n",
       " 'total',\n",
       " 'cry',\n",
       " 'disney',\n",
       " 'business',\n",
       " 'towards',\n",
       " 'teen',\n",
       " 'win',\n",
       " 'plan',\n",
       " 'company',\n",
       " 'front',\n",
       " 'fear',\n",
       " 'remake',\n",
       " 'joe',\n",
       " 'worked',\n",
       " 'award',\n",
       " 'brain',\n",
       " 'rich',\n",
       " 'incredibly',\n",
       " 'earlier',\n",
       " 'shoot',\n",
       " 'master',\n",
       " 'list',\n",
       " 'dumb',\n",
       " 'project',\n",
       " 'unique',\n",
       " 'cat',\n",
       " 'realize',\n",
       " 'powerful',\n",
       " 'dramatic',\n",
       " 'older',\n",
       " 'following',\n",
       " 'era',\n",
       " 'term',\n",
       " 'success',\n",
       " 'costume',\n",
       " 'plenty',\n",
       " 'directing',\n",
       " 'ask',\n",
       " 'amount',\n",
       " 'various',\n",
       " 'creepy',\n",
       " 'band',\n",
       " 'appear',\n",
       " 'brings',\n",
       " 'memory',\n",
       " 'italian',\n",
       " 'century',\n",
       " 'store',\n",
       " 'co',\n",
       " 'admit',\n",
       " 'apart',\n",
       " 'political',\n",
       " 'train',\n",
       " 'blue',\n",
       " 'law',\n",
       " 'fairly',\n",
       " 'leading',\n",
       " 'spent',\n",
       " 'la',\n",
       " 'portrayed',\n",
       " 'trouble',\n",
       " 'creature',\n",
       " 'telling',\n",
       " 'outside',\n",
       " 'wasted',\n",
       " 'fighting',\n",
       " 'william',\n",
       " 'fails',\n",
       " 'chase',\n",
       " 'spirit',\n",
       " 'portrayal',\n",
       " 'deserves',\n",
       " 'plain',\n",
       " 'throw',\n",
       " 'office',\n",
       " 'drive',\n",
       " 'meant',\n",
       " 'concept',\n",
       " 'player',\n",
       " 'create',\n",
       " 'appearance',\n",
       " 'attack',\n",
       " 'ability',\n",
       " 'expecting',\n",
       " 'missing',\n",
       " 'language',\n",
       " 'mistake',\n",
       " 'manages',\n",
       " 'channel',\n",
       " 'ended',\n",
       " 'recently',\n",
       " 'cute',\n",
       " 'hole',\n",
       " 'agree',\n",
       " 'stick',\n",
       " 'ghost',\n",
       " 'caught',\n",
       " 'talented',\n",
       " 'depth',\n",
       " 'large',\n",
       " 'odd',\n",
       " 'unlike',\n",
       " 'hardly',\n",
       " 'produced',\n",
       " 'pace',\n",
       " 'clever',\n",
       " 'soul',\n",
       " 'door',\n",
       " 'flat',\n",
       " 'laughing',\n",
       " 'color',\n",
       " 'nudity',\n",
       " 'public',\n",
       " 'tension',\n",
       " 'extra',\n",
       " 'potential',\n",
       " 'indian',\n",
       " 'culture',\n",
       " 'casting',\n",
       " 'created',\n",
       " 'missed',\n",
       " 'pull',\n",
       " 'wrote',\n",
       " 'married',\n",
       " 'control',\n",
       " 'pure',\n",
       " 'waiting',\n",
       " 'van',\n",
       " 'scott',\n",
       " 'science',\n",
       " 'filled',\n",
       " 'cold',\n",
       " 'respect',\n",
       " 'sweet',\n",
       " 'incredible',\n",
       " 'mentioned',\n",
       " 'sadly',\n",
       " 'visual',\n",
       " 'familiar',\n",
       " 'million',\n",
       " 'slightly',\n",
       " 'speak',\n",
       " 'considering',\n",
       " 'gang',\n",
       " 'died',\n",
       " 'answer',\n",
       " 'hair',\n",
       " 'us',\n",
       " 'popular',\n",
       " 'catch',\n",
       " 'purpose',\n",
       " 'post',\n",
       " 'anti',\n",
       " 'meaning',\n",
       " 'college',\n",
       " 'convincing',\n",
       " 'jane',\n",
       " 'social',\n",
       " 'decides',\n",
       " 'track',\n",
       " 'taste',\n",
       " 'dancing',\n",
       " 'entirely',\n",
       " 'bored',\n",
       " 'positive',\n",
       " 'critic',\n",
       " 'neither',\n",
       " 'intelligent',\n",
       " 'compared',\n",
       " 'follows',\n",
       " 'basic',\n",
       " 'former',\n",
       " 'common',\n",
       " 'lie',\n",
       " 'mad',\n",
       " 'appreciate',\n",
       " 'suddenly',\n",
       " 'genius',\n",
       " 'building',\n",
       " 'alive',\n",
       " 'camp',\n",
       " 'younger',\n",
       " 'violent',\n",
       " 'belief',\n",
       " 'yeah',\n",
       " 'road',\n",
       " 'biggest',\n",
       " 'adaptation',\n",
       " 'fake',\n",
       " 'computer',\n",
       " 'step',\n",
       " 'slasher',\n",
       " 'fiction',\n",
       " 'ride',\n",
       " 'flaw',\n",
       " 'spot',\n",
       " 'christmas',\n",
       " 'trip',\n",
       " 'recommended',\n",
       " 'successful',\n",
       " 'amusing',\n",
       " 'conclusion',\n",
       " 'exciting',\n",
       " 'trash',\n",
       " 'land',\n",
       " 'pointless',\n",
       " 'suck',\n",
       " 'teenager',\n",
       " 'aside',\n",
       " 'longer',\n",
       " 'date',\n",
       " 'pathetic',\n",
       " 'sick',\n",
       " 'shooting',\n",
       " 'tone',\n",
       " 'park',\n",
       " 'scientist',\n",
       " 'bizarre',\n",
       " 'prison',\n",
       " ...]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_new = np.array(X1)\n",
    "X1_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1_new, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(criterion='entropy', n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=300)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X1_train, y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = rfc.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833\n"
     ]
    }
   ],
   "source": [
    "score=accuracy_score(y1_test,y_pred2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_pickle(\"review_classifier_model\", rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv==x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.7916073 ,  0.39673504, -0.7527301 , -0.88069326,  0.23557387,\n",
       "       -1.3942405 ,  0.3816136 ,  0.18450025, -0.8303827 ,  0.82364345,\n",
       "       -1.1227267 , -0.85762465,  0.29456764,  0.581786  ,  0.3304755 ,\n",
       "        0.3067833 , -1.3680676 , -1.760744  , -1.4201983 , -0.683725  ,\n",
       "       -2.2025824 , -1.3758914 ,  1.6227067 , -0.6203595 ,  0.14728004,\n",
       "       -0.8543858 , -1.9281954 , -0.9384264 , -0.23511815,  0.25570595,\n",
       "        2.3697062 , -1.1228553 ,  0.6284109 , -0.65249354,  1.971913  ,\n",
       "       -1.314398  ,  0.17824186, -0.66040516,  0.07912144, -1.1437658 ,\n",
       "        1.1693095 , -0.33764693,  0.27976593, -0.7348727 ,  0.3053981 ,\n",
       "       -0.70772696,  0.69819546, -1.228625  , -1.5835164 , -0.81319404,\n",
       "        0.57749426,  1.6408365 , -0.0219146 ,  1.3130064 , -1.0657222 ,\n",
       "        1.6303095 ,  0.00499786,  0.53303236, -3.3025374 , -1.0137596 ,\n",
       "       -0.53677446, -0.9982146 , -1.0254186 ,  1.1907377 , -0.26748604,\n",
       "       -0.6581822 ,  1.2045606 ,  0.05325426,  1.4532279 , -0.2259317 ,\n",
       "       -0.10079535, -1.0485243 , -2.236956  ,  0.4490439 ,  0.33204016,\n",
       "        0.33427903,  1.6425092 ,  1.904161  , -0.97842425, -0.8459796 ,\n",
       "       -0.8559469 , -0.32629228, -1.6709982 , -0.9995928 , -1.0525436 ,\n",
       "        0.7818355 ,  0.7949094 ,  2.7965758 ,  1.2234079 , -0.6091691 ,\n",
       "       -2.4141998 ,  0.10696498,  1.9404942 , -0.21147189, -0.48520488,\n",
       "        2.3494036 , -1.3965597 ,  0.2228631 ,  0.3402816 , -0.90844434],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.array(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        0\n",
       "4        1\n",
       "        ..\n",
       "49995    1\n",
       "49996    0\n",
       "49997    0\n",
       "49998    0\n",
       "49999    0\n",
       "Name: sentiment, Length: 50000, dtype: uint8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'] = pd.get_dummies(data['sentiment'],drop_first=True)\n",
    "data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "review_imdb_detect_model = RandomForestClassifier()\n",
    "review_imdb_detect_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=review_imdb_detect_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8404\n"
     ]
    }
   ],
   "source": [
    "score=accuracy_score(y_test,y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators':[100,200,300],\n",
    "                     'criterion':['gini','entropy']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] criterion=gini, n_estimators=100 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. criterion=gini, n_estimators=100, total=  48.8s\n",
      "[CV] criterion=gini, n_estimators=100 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   48.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. criterion=gini, n_estimators=100, total=  46.1s\n",
      "[CV] criterion=gini, n_estimators=100 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=100, total=  46.2s\n",
      "[CV] criterion=gini, n_estimators=100 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=100, total=  44.7s\n",
      "[CV] criterion=gini, n_estimators=100 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=100, total=  44.9s\n",
      "[CV] criterion=gini, n_estimators=200 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=200, total= 1.5min\n",
      "[CV] criterion=gini, n_estimators=200 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=200, total= 1.5min\n",
      "[CV] criterion=gini, n_estimators=200 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=200, total= 1.6min\n",
      "[CV] criterion=gini, n_estimators=200 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=200, total= 1.6min\n",
      "[CV] criterion=gini, n_estimators=200 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=200, total= 1.7min\n",
      "[CV] criterion=gini, n_estimators=300 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=300, total= 2.5min\n",
      "[CV] criterion=gini, n_estimators=300 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=300, total= 2.3min\n",
      "[CV] criterion=gini, n_estimators=300 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=300, total= 2.2min\n",
      "[CV] criterion=gini, n_estimators=300 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=300, total= 2.4min\n",
      "[CV] criterion=gini, n_estimators=300 ................................\n",
      "[CV] ................. criterion=gini, n_estimators=300, total= 2.3min\n",
      "[CV] criterion=entropy, n_estimators=100 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=100, total= 1.1min\n",
      "[CV] criterion=entropy, n_estimators=100 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=100, total= 1.1min\n",
      "[CV] criterion=entropy, n_estimators=100 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=100, total= 1.1min\n",
      "[CV] criterion=entropy, n_estimators=100 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=100, total= 1.1min\n",
      "[CV] criterion=entropy, n_estimators=100 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=100, total= 1.1min\n",
      "[CV] criterion=entropy, n_estimators=200 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=200, total= 2.1min\n",
      "[CV] criterion=entropy, n_estimators=200 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=200, total= 2.2min\n",
      "[CV] criterion=entropy, n_estimators=200 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=200, total= 2.1min\n",
      "[CV] criterion=entropy, n_estimators=200 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=200, total= 2.1min\n",
      "[CV] criterion=entropy, n_estimators=200 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=200, total= 2.1min\n",
      "[CV] criterion=entropy, n_estimators=300 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=300, total= 3.2min\n",
      "[CV] criterion=entropy, n_estimators=300 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=300, total=18.0min\n",
      "[CV] criterion=entropy, n_estimators=300 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=300, total= 3.3min\n",
      "[CV] criterion=entropy, n_estimators=300 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=300, total= 3.3min\n",
      "[CV] criterion=entropy, n_estimators=300 .............................\n",
      "[CV] .............. criterion=entropy, n_estimators=300, total= 3.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 70.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridSearchCV(RandomForestClassifier(),params,cv=5,verbose=2)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=300)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8471"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(criterion='entropy', n_estimators=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=300)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.842\n"
     ]
    }
   ],
   "source": [
    "score=accuracy_score(y_test,y_pred1)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,open('RFC_NPL.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train = np.array(y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = Sequential()\n",
    "ann.add(Dense(8, input_dim = input_dim, kernel_initializer='normal', activation='relu'))\n",
    "ann.add(Dense(5, activation = \"relu\", kernel_initializer='normal'))\n",
    "ann.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer = Adam(learning_rate = 0.001),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000029BCEF69828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000029BCEF69828> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2670/2680 [============================>.] - ETA: 0s - loss: 0.3775 - accuracy: 0.8377WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000029BCF0CB4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000029BCF0CB4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2680/2680 [==============================] - 35s 8ms/step - loss: 0.3773 - accuracy: 0.8377 - val_loss: 0.3266 - val_accuracy: 0.8639\n",
      "Epoch 2/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3340 - accuracy: 0.8590 - val_loss: 0.3239 - val_accuracy: 0.8644\n",
      "Epoch 3/20\n",
      "2680/2680 [==============================] - 15s 6ms/step - loss: 0.3289 - accuracy: 0.8621 - val_loss: 0.3210 - val_accuracy: 0.8650\n",
      "Epoch 4/20\n",
      "2680/2680 [==============================] - 15s 6ms/step - loss: 0.3254 - accuracy: 0.8613 - val_loss: 0.3210 - val_accuracy: 0.8656\n",
      "Epoch 5/20\n",
      "2680/2680 [==============================] - 15s 6ms/step - loss: 0.3241 - accuracy: 0.8626 - val_loss: 0.3154 - val_accuracy: 0.8689\n",
      "Epoch 6/20\n",
      "2680/2680 [==============================] - 15s 6ms/step - loss: 0.3207 - accuracy: 0.8638 - val_loss: 0.3286 - val_accuracy: 0.8616\n",
      "Epoch 7/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3200 - accuracy: 0.8651 - val_loss: 0.3196 - val_accuracy: 0.8660\n",
      "Epoch 8/20\n",
      "2680/2680 [==============================] - 15s 5ms/step - loss: 0.3185 - accuracy: 0.8664 - val_loss: 0.3165 - val_accuracy: 0.8677\n",
      "Epoch 9/20\n",
      "2680/2680 [==============================] - 15s 5ms/step - loss: 0.3176 - accuracy: 0.8647 - val_loss: 0.3134 - val_accuracy: 0.8690\n",
      "Epoch 10/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3165 - accuracy: 0.8662 - val_loss: 0.3135 - val_accuracy: 0.8703\n",
      "Epoch 11/20\n",
      "2680/2680 [==============================] - 15s 6ms/step - loss: 0.3157 - accuracy: 0.8676 - val_loss: 0.3154 - val_accuracy: 0.8683\n",
      "Epoch 12/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3150 - accuracy: 0.8662 - val_loss: 0.3192 - val_accuracy: 0.8655\n",
      "Epoch 13/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3144 - accuracy: 0.8659 - val_loss: 0.3142 - val_accuracy: 0.8673\n",
      "Epoch 14/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3134 - accuracy: 0.8667 - val_loss: 0.3182 - val_accuracy: 0.8662\n",
      "Epoch 15/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3129 - accuracy: 0.8678 - val_loss: 0.3155 - val_accuracy: 0.8684\n",
      "Epoch 16/20\n",
      "2680/2680 [==============================] - 15s 5ms/step - loss: 0.3123 - accuracy: 0.8685 - val_loss: 0.3143 - val_accuracy: 0.8695\n",
      "Epoch 17/20\n",
      "2680/2680 [==============================] - 15s 6ms/step - loss: 0.3113 - accuracy: 0.8677 - val_loss: 0.3150 - val_accuracy: 0.8688\n",
      "Epoch 18/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3111 - accuracy: 0.8672 - val_loss: 0.3164 - val_accuracy: 0.8653\n",
      "Epoch 19/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3097 - accuracy: 0.8693 - val_loss: 0.3148 - val_accuracy: 0.8689\n",
      "Epoch 20/20\n",
      "2680/2680 [==============================] - 14s 5ms/step - loss: 0.3093 - accuracy: 0.8682 - val_loss: 0.3173 - val_accuracy: 0.8683\n"
     ]
    }
   ],
   "source": [
    "result = ann.fit(X1_train, y1_train,validation_split=0.33, epochs = 20, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000029BCE3DE558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x0000029BCE3DE558> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "313/313 [==============================] - 1s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "ypred3 = ann.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred3 = (ypred3 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8596\n"
     ]
    }
   ],
   "source": [
    "score=accuracy_score(np.array(y_test),ypred3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x0000029BCE4E8F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function trace_model_call.<locals>._wrapped_model at 0x0000029BCE4E8F78> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000029BCE4E8C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function canonicalize_signatures.<locals>.signature_wrapper at 0x0000029BCE4E8C18> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Assets written to: ram://0eed6634-9940-4d0d-b0d4-0c94ec24484c/assets\n"
     ]
    }
   ],
   "source": [
    "compressed_pickle(\"review_classifier_ann\", ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.save('ann.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84      4897\n",
      "           1       0.85      0.83      0.84      5103\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 'Jesus Christ! Not only does this film reach our expectations as a film, it surpasses them. There are quite a few things I like about this film so this review may take a while to read.Firstly, the Russo brothers obviously show deep appreciation for the MCU and the effort they put in for this film is clearly visible with so many awesomely-handled pivotal moments in the films plot. It seems that many of the MCUs best films are directed by Anthony and Joe Russo and that doesnt surprise me one bit.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(z):    \n",
    "    corpus1=[]\n",
    "    for i in range(0,1):\n",
    "        review = re.sub('[^a-zA-Z]', ' ', z)\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "        review = ' '.join(review)\n",
    "        corpus1.append(review)\n",
    "    words2 = []\n",
    "    for i in corpus1:\n",
    "        sent_token = sent_tokenize(i)\n",
    "        for j in sent_token:\n",
    "            words2.append(simple_preprocess(j))\n",
    "    x = np.array( np.mean([model1.wv[word] for word in words2[0] if word in model1.wv.index_to_key],axis=0))\n",
    "    ans = ann.predict(x.reshape(1,100))\n",
    "    ans = (ans > 0.5)\n",
    "    return ans[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_analysis(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=\"Im not sure what accomplished director/producer/cinematographer Joshua Caldwell was thinking taking on this project.This film has got to be the epitome of terrible writing and should be a classroom example of 'what not to do' when writing a screenplay. Why would Joshua take on (clearly) amateur writer Adam Gaines script is beyond me. Even his good directing and excellent cinematography could not save this disaster.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=\"This film is an emotional rollercoaster with some of the coolest superhero plot lines ever drawn up. It's straight up the most epic Marvel film that will probably ever be created. I don't see how Marvel could ever top this, but getting to see these characters all together at least one last time was a reward all on its own.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dill\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Installing collected packages: dill\n",
      "Successfully installed dill-0.3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(review_imdb_detect_model,open('npl_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2 , -0.25, -1.35, -0.23,  0.92,  0.79,  1.46,  0.33, -0.06,\n",
       "        0.2 ,  0.68,  1.75, -1.39,  1.92,  0.07, -0.35,  1.22,  1.06,\n",
       "       -3.66,  0.18, -0.08,  0.24,  2.13, -0.57,  1.12, -0.23, -0.79,\n",
       "        1.61, -0.25,  1.5 , -0.57, -1.85,  0.91, -1.21, -1.37,  0.63,\n",
       "        1.79,  0.36,  2.45,  1.81,  0.39, -0.47,  0.07, -0.57,  0.36,\n",
       "        0.63,  0.14,  0.97, -1.17, -0.65,  1.19, -0.44, -0.51, -0.92,\n",
       "       -0.4 , -1.63,  0.65, -1.18, -0.52,  0.69,  0.52, -0.68, -1.1 ,\n",
       "       -0.72, -0.21,  1.4 ,  0.28, -0.07, -0.52,  0.83,  0.93,  1.6 ,\n",
       "       -2.6 ,  1.24,  2.18, -1.61, -0.11,  1.76,  0.83, -0.56, -1.74,\n",
       "        1.08,  0.24,  0.98, -0.64,  0.21,  1.45, -2.26, -2.14,  0.36,\n",
       "        1.07, -1.4 ,  0.7 , -0.33,  1.62,  0.2 ,  0.44, -0.31, -0.97,\n",
       "        0.6 ], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
